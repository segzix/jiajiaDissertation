%---------------------------------------------------------------------------%
%-                                                                         -%
%-                             Bibliography                                -%
%-                                                                         -%
%---------------------------------------------------------------------------%
@book{wikibook2014latex,
    title={http://en.wikibooks.org/wiki/LaTeX},
    author={Wikibook},
    year={2014},
    publisher={On-line Resources}
}
@book{lamport1986document,
    title={Document Preparation System},
    author={Lamport, Leslie},
    year={1986},
    publisher={Addison-Wesley Reading, MA}
}
@article{chen2005zhulu,
    title={著录文后参考文献的规则及注意事项},
    author={陈浩元},
    key={Chen Hao Yuan},
    journal={编辑学报},
    volume={17},
    number={6},
    pages={413--415},
    year={2005}
}
@book{chu2004tushu,
    title={图书馆数字参考咨询服务研究},
    author={初景利},
    key={Chu Jing Li},
    year={2004},
    address={北京},
    publisher={北京图书馆出版社}
}
@article{stamerjohanns2009mathml,
    title={{MathML}-aware article conversion from {LaTeX}},
    author={Stamerjohanns, Heinrich and Ginev, Deyan and David, Catalin and Misev, Dimitar and Zamdzhiev, Vladimir and Kohlhase, Michael},
    journal={Towards a Digital Mathematics Library},
    volume={16},
    number={2},
    pages={109--120},
    year={2009},
    publisher={Masaryk University Press}
}
@article{betts2005aging,
    title={Aging reduces center-surround antagonism in visual motion processing},
    author={Betts, Lisa R and Taylor, Christopher P},
    journal={Neuron},
    volume={45},
    number={3},
    pages={361--366},
    year={2005},
    publisher={Elsevier}
}

@article{bravo1990comparative,
    title={Comparative study of visual inter and intrahemispheric cortico-cortical connections in five native Chilean rodents},
    author={Bravo, Hermes and Olavarria, Jaime},
    journal={Anatomy and embryology},
    volume={181},
    number={1},
    pages={67--73},
    year={1990},
    publisher={Springer}
}
@book{hls2012jinji,
    author       = {哈里森·沃尔德伦},
    key          = {Haliseng Woerdelun},
    translator   = {谢远涛},
    title        = {经济数学与金融数学},
    address      = {北京},
    publisher    = {中国人民大学出版社},
    year         = {2012},
    pages        = {235--236},
}
@proceedings{niu2013zonghe,
    editor       = {牛志明 and 斯温兰德 and 雷光春},
    key          = {Niu Zhi Ming Siwenlande Lei Guang Chun},
    title        = {综合湿地管理国际研讨会论文集},
    address      = {北京},
    publisher    = {海洋出版社},
    year         = {2013},
}
@incollection{chen1980zhongguo,
    author       = {陈晋镳 and 张惠民 and 朱士兴 and 赵震 and
        王振刚},
    key          = {Chen Jing Ao Zhang Hui Ming Zhu Shi Xing Zhao Zhen Wang Zhen Gang},
    title        = {蓟县震旦亚界研究},
    editor       = {中国地质科学院天津地质矿产研究所},
    booktitle    = {中国震旦亚界},
    address      = {天津},
    publisher    = {天津科学技术出版社},
    year         = {1980},
    pages        = {56--114},
}
@article{yuan2012lana,
    author       = {袁训来 and 陈哲 and 肖书海},
    key          = {Yuan xun lai Chen zhe Xiao shu Hai},
    title        = {蓝田生物群: 一个认识多细胞生物起源和早期演化的新窗口 -- 篇一},
    journal      = {科学通报},
    year         = {2012},
    volume       = {57},
    number       = {34},
    pages        = {3219},
}
@article{yuan2012lanb,
    author       = {袁训来 and 陈哲 and 肖书海},
    key          = {Yuan xun lai Chen zhe Xiao shu Hai},
    title        = {蓝田生物群: 一个认识多细胞生物起源和早期演化的新窗口 -- 篇二},
    journal      = {科学通报},
    year         = {2012},
    volume       = {57},
    number       = {34},
    pages        = {3219},
}
@article{yuan2012lanc,
    author       = {袁训来 and 陈哲 and 肖书海},
    key          = {Yuan xun lai Chen zhe Xiao shu Hai},
    title        = {蓝田生物群: 一个认识多细胞生物起源和早期演化的新窗口 -- 篇三},
    journal      = {科学通报},
    year         = {2012},
    volume       = {57},
    number       = {34},
    pages        = {3219},
}
@article{walls2013drought,
    author       = {Walls, Susan C. and Barichivich, William J. and Brown, Mary
        E.},
    title        = {Drought, deluge and declines: the impact of precipitation
        extremes on amphibians in a changing climate},
    journal      = {Biology},
    year         = {2013},
    volume       = {2},
    number       = {1},
    pages        = {399--418},
    urldate      = {2013-11-04},
    url          = {http://www.mdpi.com/2079-7737/2/1/399},
    doi          = {10.3390/biology2010399},
}
@article{Bohan1928,
    author = { ボハン, デ},
    title = { 過去及び現在に於ける英国と会 },
    journal = { 日本時報 },
    year = { 1928 },
    volume = { 17 },
    pages = { 5-9 },
    edition = { 9 },
    hyphenation = { japanese },
    language = { japanese }
}

@article{Dubrovin1906,
    author = { Дубровин, А. И },
    title = { Открытое письмо Председателя Главного Совета Союза Русского Народа Санкт-Петербургскому Антонию, Первенствующему члену Священного Синода },
    journal = { Вече },
    year = { 1906 },
    volume = {  },
    edition = { 97 },
    month = { 7 дек. 1906 },
    pages = { 1-3 },
    hyphenation = { russian },
    language = { russian }
}
%---------------------------------------------------------------------------%

%---------------------------------------------------------------------------%
%-                                                                         -%
%-                             Bibliography                                -%
%-                                                                         -%
%---------------------------------------------------------------------------%
@book{huweiwu2001sma,
  title={共享存储系统结构},
  author={胡伟武},
  publisher={高等教育出版社},
  year={2001},
}

@book{huweiwu2024ca,
title={计算机体系结构基础},
author={胡伟武等},
publisher={机械工业出版社},
year={2024},
}

@INBOOK{Ghosh2023dsm,
  author={Ghosh, Ratan K. and Ghosh, Hiranmay},
  booktitle={Distributed Systems: Theory and Applications}, 
  title={Distributed Shared Memory}, 
  publisher={},
  year={2023},
  volume={},
  number={},
  pages={337-369},
  keywords={Multicore processing;Programming;Switches;Program processors;Computer architecture;Technological innovation;Message systems},
  doi={10.1002/9781119825968.ch13}
}

@ARTICLE{nitzberg1991dsm,
  author={Nitzberg, B. and Lo, V.},
  journal={Computer}, 
  title={Distributed shared memory: a survey of issues and algorithms}, 
  year={1991},
  volume={24},
  number={8},
  pages={52-60},
  keywords={Data handling;Distributed computing;Memory management},
  doi={10.1109/2.84877}
}

@phdthesis{likai1986svm,
  author       = {Li, K},
  title        = {Shared virtual memory on loosely coupled multiprocessors},
  annote       = {This dissertation demonstrates that parallel programs using shared virtual memory on loosely coupled multiprocessors can achieve orders-of-magnitude speedups over a uniprocessor and that it is practical to implement a shared virtual memory on existing architectures. Virtual memory has proven benefits. Today, almost every high-performance sequential computer has one. While one can easily imagine how virtual memory would be incorporated into a shared-memory parallel machine, on a multiprocessor in which the physical memory is distributed, the implementations of virtual memory is not obvious. Algorithms are presented for solving memory-coherence problems in a shared virtual memory on loosely coupled multiprocessors. It discusses basic mechanisms for process scheduling (including process migration) and memory management. Many different strategies are presented, analyzed, and compared, and a few of the most viable ones are chosen for implementation. A prototype system, IVY, was implemented on a local area network of Apollo workstations.},
  url          = {https://www.osti.gov/biblio/6674218},
  place        = {United States},
  publisher    = {Yale Univ.,New Haven, CT},
  year         = {1986},
  month        = {01},
  school       = {Yale Univ}
}

@article{likai1988ivy,
    title = "IVY: a shared virtual memory system for parallel computing",
    abstract = "A shared-virtual-memory system is described which can provide a virtual address space shared among all processors in a loosely-coupled multiprocessor. It is shown that such a memory can solve many problems in message-passing systems on loosely-coupled multiprocessors. The design and implementation are described of a prototype shared-virtual-memory system, IVY, implemented on an Apollo ring network. The experiments on the prototype system show that parallel programs using a shared virtual memory yield almost linear and ocassionally superlinear speedups and that it is practical to implement such a system on existing architectures.",

    author = "Kai Li",
    year = "1988",
    language = "English (US)",
    volume = "2",
    pages = "94--101",
    journal = "Proceedings of the International Conference on Parallel Processing",
    issn = "0190-3918",
    publisher = "Institute of Electrical and Electronics Engineers Inc.",
}

@ARTICLE{amza1996treadmarks,
  author={Amza, C. and Cox, A.L. and Dwarkadas, S. and Keleher, P. and Honghui Lu and Rajamony, R. and Weimin Yu and Zwaenepoel, W.},
  journal={Computer}, 
  title={TreadMarks: shared memory computing on networks of workstations}, 
  year={1996},
  volume={29},
  number={2},
  pages={18-28},
  keywords={Computer networks;Workstations;Hardware;Programming profession;Message passing;Data structures;Partitioning algorithms;Programming environments;Delay;Protection},
  doi={10.1109/2.485843}}

@InProceedings{1999huweiwuJIAJIA,
    author="Hu, Weiwu
    and Shi, Weisong
    and Tang, Zhimin",
    editor="Sloot, Peter
    and Bubak, Marian
    and Hoekstra, Alfons
    and Hertzberger, Bob",
    title="JIAJIA: A software DSM system based on a new cache coherence protocol",
    booktitle="High-Performance Computing and Networking",
    year="1999",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="461--472",
    abstract="This paper describes design and evaluation of a software distributed shared memory (DSM) system called JIAJIA. JIAJIA is a home-based software DSM system in which physical memories of multiple computers are combined to form a larger shared space. It implements the lock-based cache coherence protocol which totally eliminates directory and maintains coherence through accessing write notices kept on the lock. Our experiments with some widely accepted DSM benchmarks such as SPLASH2 program suite and NAS Parallel Benchmarks indicate that, compared to recent software DSMs such as CVM, higher performance is achieved by JIAJIA. Besides, JIAJIA can solve large problems that cannot be solved by other software DSMs due to memory size limitation.",
    isbn="978-3-540-48933-7"
}

@article{10.1145/17356.17406,
author = {Dubois, M. and Scheurich, C. and Briggs, F.},
title = {Memory access buffering in multiprocessors},
year = {1986},
issue_date = {May 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/17356.17406},
doi = {10.1145/17356.17406},
abstract = {In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access latency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multiprocessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement resulting from buffering.},
journal = {SIGARCH Comput. Archit. News},
month = may,
pages = {434–442},
numpages = {9}
}

@ARTICLE{lamport1979,
  author={Lamport},
  journal={IEEE Transactions on Computers}, 
  title={How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs}, 
  year={1979},
  volume={C-28},
  number={9},
  pages={690-691},
  keywords={Computer design;concurrent computing;hardware correctness;multiprocessing;parallel processing},
  doi={10.1109/TC.1979.1675439}
}

@inproceedings{1986weakconsistency,
author = {Dubois, M. and Scheurich, C. and Briggs, F.},
title = {Memory access buffering in multiprocessors},
year = {1986},
isbn = {081860719X},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access latency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multiprocessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement resulting from buffering.},
booktitle = {Proceedings of the 13th Annual International Symposium on Computer Architecture},
pages = {434–442},
numpages = {9},
location = {Tokyo, Japan},
series = {ISCA '86}
}

@INPROCEEDINGS{1990releaseconsistency,
  author={Gharachorloo, K. and Lenoski, D. and Laudon, J. and Gibbons, P. and Gupta, A. and Hennessy, J.},
  booktitle={[1990] Proceedings. The 17th Annual International Symposium on Computer Architecture}, 
  title={Memory consistency and event ordering in scalable shared-memory multiprocessors}, 
  year={1990},
  volume={},
  number={},
  pages={15-26},
  keywords={Programming profession;Delay;Pipeline processing;Out of order;Hardware;Intelligent networks;Computer networks;Distributed computing;Laboratories;Multiprocessor interconnection networks},
  doi={10.1109/ISCA.1990.134503}}

@article{bennett1990munin,
author = {Bennett, J. K. and Carter, J. B. and Zwaenepoel, W.},
title = {Munin: distributed shared memory based on type-specific memory coherence},
year = {1990},
issue_date = {Mar. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/99164.99182},
doi = {10.1145/99164.99182},
abstract = {We are developing Munin, a system that allows programs written for shared memory multiprocessors to be executed efficiently on distributed memory machines. Munin attempts to overcome the architectural limitations of shared memory machines, while maintaining their advantages in terms of ease of programming. Our system is unique in its use of loosely coherent memory, based on the partial order specified by a shared memory parallel program, and in its use of type-specific memory coherence. Instead of a single memory coherence mechanism for all shared data objects, Munin employs several different mechanisms, each appropriate for a different class of shared data object. These type-specific mechanisms are part of a runtime system that accepts hints from the user or the compiler to determine the coherence mechanism to be used for each object. This paper focuses on the design and use of Munin's memory coherence mechanisms, and compares our approach to previous work in this area.},
journal = {SIGPLAN Not.},
month = feb,
pages = {168–176},
numpages = {9}
}


@article{Iftode1998sc,
  author    = {L. Iftode and J. P. Singh and K. Li},
  title     = {Scope Consistency: A Bridge between Release Consistency and Entry Consistency},
  journal   = {Theory of Computing Systems},
  year      = {1998},
  volume    = {31},
  number    = {4},
  pages     = {451--473},
  doi       = {10.1007/s002240000097},
  url       = {https://doi.org/10.1007/s002240000097},
  issn      = {1433-0490},
  abstract  = {Systems that maintain coherence at large granularity, such as shared virtual memory systems, suffer from false sharing and extra communication. Relaxed memory consistency models have been used to alleviate these problems, but at a cost in programming complexity. Release Consistency (RC) and Lazy Release Consistency (LRC) are accepted to offer a reasonable tradeoff between performance and programming complexity. Entry Consistency (EC) offers a more relaxed consistency model, but it requires explicit association of shared data objects with synchronization variables. The programming burden of providing such associations can be substantial.}
}


@article{huweiwu1998lsc,
  title = {A Lock-Based Cache Coherence Protocol for Scope Consistency},
  author = {Hu, Weiwu and Shi, Weisong and Tang, Zhimin and Li, Ming},
  year = {1998},
  month = mar,
  journal = {Journal of Computer Science and Technology},
  volume = {13},
  number = {2},
  pages = {97--109},
  issn = {1000-9000, 1860-4749},
  doi = {10.1007/BF02946599},
  urldate = {2025-02-10},
  abstract = {Directory protocols are widely adopted to maintain cache coherence of distributed shared memory multiprocessors. Although scalable to a certain extent, directory protocols are complex:enough to prevent it from being used in very large scale multiprocessors with tens of thousands of nodes. This paper proposes a lock-based cache coherence protocol for scope consistency. It does not rely on directory information to maintain cache coherence. Instead, cache coherence is maintained through requiring the releasing processor of a lock to store all write-notices generated in the associated critical section to the lock and the acquiring processor invalidates or updates its locally cached data copies according to the write notices of the lock. To evaluate the performance of the lock-based cache coherence protocol, a software DSM system named JIAJIA is built on network of workstations. Besides the lock-based cache coherence protocol, JIAJIA also characterizes itself with its shared memory organization scheme which combines the physical memories of multiple workstations to form a large shared space. Performance measurements with SPLASH2 program suite and NAS benchmarks indicate that, compared to recent SVM systems such as CVM, higher speedup is achieved by JIAJIA. Besides, JIAJIA can solve large scale problems that cannot be solved by other SVM systems due to memory size limitation.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  annotation = {TLDR: Performance measurements with SPLASH2 program suite and NAS benchmarks indicate that, compared to recent SVM systems such as CVM, higher speedup is achieved by JIAJIA.},
  file = {/home/yongy/Zotero/storage/ZT9ERLJT/Hu et al. - 1998 - A lock-based cache coherence protocol for scope consistency.pdf}
}




@misc{cudennec2020softwaredistributedsharedmemoryheterogeneous,
      title={Software-Distributed Shared Memory for Heterogeneous Machines: Design and Use Considerations}, 
      author={Loïc Cudennec},
      year={2020},
      eprint={2009.01507},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2009.01507}, 
}

@inproceedings{zaharia2010spark,
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
title = {Spark: cluster computing with working sets},
year = {2010},
publisher = {USENIX Association},
address = {USA},
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
booktitle = {Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing},
pages = {10},
numpages = {1},
location = {Boston, MA},
series = {HotCloud'10}
}

@INPROCEEDINGS{vasava2015sdsm-multiprocessor,
  author={Vasava, Hemant D. and Rathod, Jagdish M.},
  booktitle={2015 International Conference on Communications and Signal Processing (ICCSP)}, 
  title={Software based distributed shared memory (DSM) model using shared variables between multiprocessors}, 
  year={2015},
  volume={},
  number={},
  pages={1431-1435},
  keywords={Software;Reliability;Computers;Computer architecture;Algorithm design and analysis;Hardware;Sockets;Distributed shared memory (DSM);Interprocess communication (IPC);Instruction level parallelism (ILP);Very long Instruction word(VLIW)},
  doi={10.1109/ICCSP.2015.7322749}}

@inproceedings{kaxiras2015argodsm,
author = {Kaxiras, Stefanos and Klaftenegger, David and Norgren, Magnus and Ros, Alberto and Sagonas, Konstantinos},
title = {Turning Centralized Coherence and Distributed Critical-Section Execution on their Head: A New Approach for Scalable Distributed Shared Memory},
year = {2015},
isbn = {9781450335508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749246.2749250},
doi = {10.1145/2749246.2749250},
abstract = {A coherent global address space in a distributed system enables shared memory programming in a much larger scale than a single multicore or a single SMP. Without dedicated hardware support at this scale, the solution is a software distributed shared memory (DSM) system. However, traditional approaches to coherence (centralized via "active" home-node directories) and critical-section execution (distributed across nodes and cores) are inherently unfit for such a scenario. Instead, it is crucial to make decisions locally and avoid the long latencies imposed by both network and software message handlers. Likewise, synchronization is fast if it rarely involves communication with distant nodes (or even other sockets). To minimize the amount of long-latency communication required in both coherence and critical section execution, we propose a DSM system with a novel coherence protocol, and a novel hierarchical queue delegation locking approach. More specifically, we propose an approach, suitable for Data-Race-Free programs, based on self-invalidation, self-downgrade, and passive data classification directories that require no message handlers, thereby incurring no extra latency. For fast synchronization we extend Queue Delegation Locking to execute critical sections in large batches on a single core before passing execution along to other cores, sockets, or nodes, in that hierarchical order. The result is a software DSM system called Argo which localizes as many decisions as possible and allows high parallel performance with little overhead on synchronization when compared to prior DSM implementations.},
booktitle = {Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {3–14},
numpages = {12},
location = {Portland, Oregon, USA},
series = {HPDC '15}
}

@article{cai2018gam,
author = {Cai, Qingchao and Guo, Wentian and Zhang, Hao and Agrawal, Divyakant and Chen, Gang and Ooi, Beng Chin and Tan, Kian-Lee and Teo, Yong Meng and Wang, Sheng},
title = {Efficient distributed memory management with RDMA and caching},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236209},
doi = {10.14778/3236187.3236209},
abstract = {Recent advancements in high-performance networking interconnect significantly narrow the performance gap between intra-node and inter-node communications, and open up opportunities for distributed memory platforms to enforce cache coherency among distributed nodes. To this end, we propose GAM, an efficient distributed in-memory platform that provides a directory-based cache coherence protocol over remote direct memory access (RDMA). GAM manages the free memory distributed among multiple nodes to provide a unified memory model, and supports a set of user-friendly APIs for memory operations. To remove writes from critical execution paths, GAM allows a write to be reordered with the following reads and writes, and hence enforces partial store order (PSO) memory consistency. A light-weight logging scheme is designed to provide fault tolerance in GAM. We further build a transaction engine and a distributed hash table (DHT) atop GAM to show the ease-of-use and applicability of the provided APIs. Finally, we conduct an extensive micro benchmark to evaluate the read/write/lock performance of GAM under various workloads, and a macro benchmark against the transaction engine and DHT. The results show the superior performance of GAM over existing distributed memory platforms.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1604–1617},
numpages = {14}
}

@inproceedings{cudennec2019,
author = {Cudennec, Lo\"{\i}c},
title = {Merging the Publish-Subscribe Pattern with the Shared Memory Paradigm},
year = {2019},
isbn = {978-3-030-10548-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-10549-5_37},
doi = {10.1007/978-3-030-10549-5_37},
abstract = {Heterogeneous distributed architectures require high-level abstractions to ease the programmability and efficiently manage resources. Both the publish-subscribe and the shared memory models offer such abstraction. However they are intended to be used in different application contexts. In this paper we propose to merge these two models into a new one. It benefits from the rigorous cache coherence management of the shared memory and the ability to cope with dynamic large-scale environment of the publish-subscribe model. The publish-subscribe mechanisms have been implemented within a distributed shared memory system and tested using an heterogeneous micro-server.},
booktitle = {Euro-Par 2018: Parallel Processing Workshops: Euro-Par 2018 International Workshops, Turin, Italy, August 27-28, 2018, Revised Selected Papers},
pages = {469–480},
numpages = {12},
keywords = {Heterogeneous computing, Publish-Subscribe, S-DSM},
location = {Turin, Italy}
}

@inproceedings {haoranma2024drust,
author = {Haoran Ma and Yifan Qiao and Shi Liu and Shan Yu and Yuanjiang Ni and Qingda Lu and Jiesheng Wu and Yiying Zhang and Miryung Kim and Harry Xu},
title = {{DRust}: {Language-Guided} Distributed Shared Memory with Fine Granularity, Full Transparency, and Ultra Efficiency},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {97--115},
url = {https://www.usenix.org/conference/osdi24/presentation/ma-haoran},
publisher = {USENIX Association},
month = jul
}

@inproceedings{dragojevi2014farm,
author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
title = {FaRM: fast remote memory},
year = {2014},
isbn = {9781931971096},
publisher = {USENIX Association},
address = {USA},
abstract = {We describe the design and implementation of FaRM, a new main memory distributed computing platform that exploits RDMA to improve both latency and throughput by an order of magnitude relative to state of the art main memory systems that use TCP/IP. FaRM exposes the memory of machines in the cluster as a shared address space. Applications can use transactions to allocate, read, write, and free objects in the address space with location transparency. We expect this simple programming model to be sufficient for most application code. FaRM provides two mechanisms to improve performance where required: lock-free reads over RDMA, and support for collocating objects and function shipping to enable the use of efficient single machine transactions. FaRM uses RDMA both to directly access data in the shared address space and for fast messaging and is carefully tuned for the best RDMA performance. We used FaRM to build a key-value store and a graph store similar to Facebook's. They both perform well, for example, a 20-machine cluster can perform 167 million key-value lookups per second with a latency of 31µs.},
booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
pages = {401–414},
numpages = {14},
location = {Seattle, WA},
series = {NSDI'14}
}

@inproceedings{Loesing2015shareddatedatabases,
author = {Loesing, Simon and Pilman, Markus and Etter, Thomas and Kossmann, Donald},
title = {On the Design and Scalability of Distributed Shared-Data Databases},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2751519},
doi = {10.1145/2723372.2751519},
abstract = {Database scale-out is commonly implemented by partitioning data across several database instances. This approach, however, has several restrictions. In particular, partitioned databases are inflexible in large-scale deployments and assume a partition-friendly workload in order to scale. In this paper, we analyze an alternative architecture design for distributed relational databases that overcomes the limitations of partitioned databases. The architecture is based on two fundamental principles: We decouple query processing and transaction management from data storage, and we share data across query processing nodes. The combination of these design choices provides scalability, elasticity, and operational flexibility without making any assumptions on the workload. As a drawback, sharing data among multiple database nodes causes synchronization overhead. To address this limitation, we introduce techniques for scalable transaction processing in shared-data environments. Specifically, we describe mechanisms for efficient data access, concurrency control, and data buffering. In combination with new hardware trends, the techniques enable performance characteristics that top state-of-the-art partitioned databases.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {663–676},
numpages = {14},
keywords = {decoupled storage, optimistic concurrency control, shared-data, transaction processing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@misc{wang2022dsmdb,
      title={The Case for Distributed Shared-Memory Databases with RDMA-Enabled Memory Disaggregation}, 
      author={Ruihong Wang and Jianguo Wang and Stratos Idreos and M. Tamer Özsu and Walid G. Aref},
      year={2022},
      eprint={2207.03027},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2207.03027}, 
}

@article{cudennec2021experimentsUA,
  title={Experiments Using a Software-Distributed Shared Memory, MPI and 0MQ over Heterogeneous Computing Resources},
  author={Lo{\"i}c Cudennec and Kods Trabelsi},
  journal={Euro-Par 2020: Parallel Processing Workshops},
  year={2021},
  volume={12480},
  pages={237 - 248},
  url={https://api.semanticscholar.org/CorpusID:232294399}
}

@article{numrich1998coarrayfortran,
author = {Numrich, Robert W. and Reid, John},
title = {Co-array Fortran for parallel programming},
year = {1998},
issue_date = {Aug. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1061-7264},
url = {https://doi.org/10.1145/289918.289920},
doi = {10.1145/289918.289920},
abstract = {Co-Array Fortran, formerly known as F--, is a small extension of Fortran 95 for parallel processing. A Co-Array Fortran program is interpreted as if it were replicated a number of times and all copies were executed asynchronously. Each copy has its own set of data objects and is termed an image. The array syntax of Fortran 95 is extended with additional trailing subscripts in square brackets to give a clear and straightforward representation of any access to data that is spread across images.References without square brackets are to local data, so code that can run independently is uncluttered. Only where there are square brackets, or where there is a procedure call and the procedure contains square brackets, is communication between images involved.There are intrinsic procedures to synchronize images, return the number of images, and return the index of the current image.We introduce the extension; give examples to illustrate how clear, powerful, and flexible it can be; and provide a technical definition.},
journal = {SIGPLAN Fortran Forum},
month = aug,
pages = {1–31},
numpages = {31}
}

@misc{coarryfortran2,
title={Coarray Fortran 2.0 At Rice University},
url={http://caf.rice.edu/} 
}


@article{Yelick1998Titanium,
  title={Titanium: A High-performance Java Dialect},
  author={Katherine A. Yelick and Luigi Semenzato and Geoff Pike and Carleton Miyamoto and Ben Liblit and Arvind Krishnamurthy and Paul N. Hilfinger and Susan L. Graham and David E. Gay and Phillip Colella and Alexander Aiken},
  journal={Concurr. Pract. Exp.},
  year={1998},
  volume={10},
  pages={825-836},
  url={https://api.semanticscholar.org/CorpusID:367644}
}

@inproceedings{bonachea2013UPC,
  title={UPC Language and Library Specifications, Version 1.3},
  author={Dan Bonachea and Gary Funck},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:59962991}
}

@INPROCEEDINGS{bachan2019upc++,
  author={Bachan, John and Baden, Scott B. and Hofmeyr, Steven and Jacquelin, Mathias and Kamil, Amir and Bonachea, Dan and Hargrove, Paul H. and Ahmed, Hadia},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={UPC++: A High-Performance Communication Framework for Asynchronous Computation}, 
  year={2019},
  volume={},
  number={},
  pages={963-973},
  keywords={Runtime;Libraries;Electronics packaging;C++ languages;Programming;Bandwidth;Program processors;Asynchronous;PGAS;RMA;RPC;Exascale},
  doi={10.1109/IPDPS.2019.00104}}



@inproceedings{Bonachea2018GASNetEX,
  title={GASNet-EX: A High-Performance, Portable Communication Library for Exascale},
  author={Dan Bonachea and Paul H. Hargrove},
  booktitle={International Workshop on Languages and Compilers for Parallel Computing},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:69926390}
}

@inproceedings{drago2014farm,
author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
title = {FaRM: fast remote memory},
year = {2014},
isbn = {9781931971096},
publisher = {USENIX Association},
address = {USA},
abstract = {We describe the design and implementation of FaRM, a new main memory distributed computing platform that exploits RDMA to improve both latency and throughput by an order of magnitude relative to state of the art main memory systems that use TCP/IP. FaRM exposes the memory of machines in the cluster as a shared address space. Applications can use transactions to allocate, read, write, and free objects in the address space with location transparency. We expect this simple programming model to be sufficient for most application code. FaRM provides two mechanisms to improve performance where required: lock-free reads over RDMA, and support for collocating objects and function shipping to enable the use of efficient single machine transactions. FaRM uses RDMA both to directly access data in the shared address space and for fast messaging and is carefully tuned for the best RDMA performance. We used FaRM to build a key-value store and a graph store similar to Facebook's. They both perform well, for example, a 20-machine cluster can perform 167 million key-value lookups per second with a latency of 31µs.},
booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
pages = {401–414},
numpages = {14},
location = {Seattle, WA},
series = {NSDI'14}
}

@inproceedings {kalia2016fasst,
author = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
title = {{FaSST}: Fast, Scalable and Simple Distributed Transactions with {Two-Sided} ({{{{{RDMA}}}}}) Datagram {RPCs}},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {185--201},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kalia},
publisher = {USENIX Association},
month = nov
}

@article{binnig2016endofslownetworks,
author = {Binnig, Carsten and Crotty, Andrew and Galakatos, Alex and Kraska, Tim and Zamanian, Erfan},
title = {The end of slow networks: it's time for a redesign},
year = {2016},
issue_date = {March 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/2904483.2904485},
doi = {10.14778/2904483.2904485},
abstract = {The next generation of high-performance networks with remote direct memory access (RDMA) capabilities requires a fundamental rethinking of the design of distributed in-memory DBMSs. These systems are commonly built under the assumption that the network is the primary bottleneck and should be avoided at all costs, but this assumption no longer holds. For instance, with InfiniBand FDR 4\texttimes{}, the bandwidth available to transfer data across the network is in the same ballpark as the bandwidth of one memory channel. Moreover, RDMA transfer latencies continue to rapidly improve as well. In this paper, we first argue that traditional distributed DBMS architectures cannot take full advantage of high-performance networks and suggest a new architecture to address this problem. Then, we discuss initial results from a prototype implementation of our proposed architecture for OLTP and OLAP, showing remarkable performance improvements over existing designs.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {528–539},
numpages = {12}
}

@inproceedings {kalia2016rdmaguidelines,
author = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
title = {Design Guidelines for High Performance {RDMA} Systems},
booktitle = {2016 USENIX Annual Technical Conference (USENIX ATC 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {437--450},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia},
publisher = {USENIX Association},
month = jun
}

@inproceedings{Gu2017infiniswap,
  title={Efficient Memory Disaggregation with Infiniswap},
  author={Juncheng Gu and Youngmoon Lee and Yiwen Zhang and Mosharaf Chowdhury and Kang G. Shin},
  booktitle={Symposium on Networked Systems Design and Implementation},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:1411997}
}

@inproceedings{li2019rdma-enabledRPC,
author = {Li, Zhaogeng and Liu, Ning and Wu, Jiaoren},
title = {Toward a Production-ready General-purpose RDMA-enabled RPC},
year = {2019},
isbn = {9781450368865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342280.3342296},
doi = {10.1145/3342280.3342296},
booktitle = {Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos},
pages = {27–29},
numpages = {3},
location = {Beijing, China},
series = {SIGCOMM Posters and Demos '19}
}



@INPROCEEDINGS{abe2003movinghomedsm,
  author={Abe, T. and Okamoto, S.},
  booktitle={2003 IEEE Pacific Rim Conference on Communications Computers and Signal Processing (PACRIM 2003) (Cat. No.03CH37490)}, 
  title={A moving home-based software DSM system}, 
  year={2003},
  volume={1},
  number={},
  pages={17-20 vol.1},
  keywords={Software systems;Access protocols;Program processors;Timing;Prototypes;Programming profession;Sampling methods},
  doi={10.1109/PACRIM.2003.1235708}
}

@inproceedings{Cheung1999AMP,
  title={A Migrating-Home Protocol for Implementing Scope Consistency Model on a Cluster of Workstations},
  author={Benny Wang-Leung Cheung and Cho-Li Wang and Kai Hwang},
  booktitle={International Conference on Parallel and Distributed Processing Techniques and Applications},
  year={1999},
  url={https://api.semanticscholar.org/CorpusID:8410350}
}

@inproceedings{tsai2017LITE,
author = {Tsai, Shin-Yeh and Zhang, Yiying},
title = {LITE Kernel RDMA Support for Datacenter Applications},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132762},
doi = {10.1145/3132747.3132762},
abstract = {Recently, there is an increasing interest in building data-center applications with RDMA because of its low-latency, high-throughput, and low-CPU-utilization benefits. However, RDMA is not readily suitable for datacenter applications. It lacks a flexible, high-level abstraction; its performance does not scale; and it does not provide resource sharing or flexible protection. Because of these issues, it is difficult to build RDMA-based applications and to exploit RDMA's performance benefits.To solve these issues, we built LITE, a Local Indirection TiEr for RDMA in the Linux kernel that virtualizes native RDMA into a flexible, high-level, easy-to-use abstraction and allows applications to safely share resources. Despite the widely-held belief that kernel bypassing is essential to RDMA's low-latency performance, we show that using a kernel-level indirection can achieve both flexibility and low-latency, scalable performance at the same time. To demonstrate the benefits of LITE, we developed several popular datacenter applications on LITE, including a graph engine, a MapReduce system, a Distributed Shared Memory system, and a distributed atomic logging system. These systems are easy to build and deliver good performance. For example, our implementation of PowerGraph uses only 20 lines of LITE code, while outperforming PowerGraph by 3.5x to 5.6x.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {306–324},
numpages = {19},
keywords = {network stack, low-latency network, indirection, RDMA},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{stuedi2014DaRPC,
author = {Stuedi, Patrick and Trivedi, Animesh and Metzler, Bernard and Pfefferle, Jonas},
title = {DaRPC: Data Center RPC},
year = {2014},
isbn = {9781450332521},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670979.2670994},
doi = {10.1145/2670979.2670994},
abstract = {Remote Procedure Call (RPC) has been the cornerstone of distributed systems since the early 80s. Recently, new classes of large-scale distributed systems running in data centers are posing extra challenges for RPC systems in terms of scaling and latency. We find that existing RPC systems make very poor usage of resources (CPU, memory, network) and are not ready to handle these upcoming workloads.In this paper we present DaRPC, an RPC framework which uses RDMA to implement a tight integration between RPC message processing and network processing in user space. DaRPC efficiently distributes computation, network resources and RPC resources across cores and memory to achieve a high aggregate throughput (2-3M ops/sec) at a very low per-request latency (10μs with iWARP). In the evaluation we show that DaRPC can boost the RPC performance of existing distributed systems in the cloud by more than an order of magnitude for both throughput and latency.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {1–13},
numpages = {13},
keywords = {Remote Procedure Call, RDMA},
location = {Seattle, WA, USA},
series = {SOCC '14}
}

@inproceedings{nelson2015grappa,
author = {Nelson, Jacob and Holt, Brandon and Myers, Brandon and Briggs, Preston and Ceze, Luis and Kahan, Simon and Oskin, Mark},
title = {Latency-tolerant software distributed shared memory},
year = {2015},
isbn = {9781931971225},
publisher = {USENIX Association},
address = {USA},
abstract = {We present Grappa, a modern take on software distributed shared memory (DSM) for in-memory data-intensive applications. Grappa enables users to program a cluster as if it were a single, large, non-uniform memory access (NUMA) machine. Performance scales up even for applications that have poor locality and input-dependent load distribution. Grappa addresses deficiencies of previous DSM systems by exploiting application parallelism, trading off latency for throughput. We evaluate Grappa with an in-memory MapReduce framework (10\texttimes{} faster than Spark [74]); a vertex-centric framework inspired by GraphLab (1.33\texttimes{} faster than native GraphLab [48]); and a relational query execution engine (12.5\texttimes{} faster than Shark [31]). All these frameworks required only 60-690 lines of Grappa code.},
booktitle = {Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference},
pages = {291–305},
numpages = {15},
location = {Santa Clara, CA},
series = {USENIX ATC '15}
}

@inproceedings {christopher2013pilaf,
author = {Christopher Mitchell and Yifeng Geng and Jinyang Li},
title = {Using {One-Sided} {RDMA} Reads to Build a Fast, {CPU-Efficient} {Key-Value} Store},
booktitle = {2013 USENIX Annual Technical Conference (USENIX ATC 13)},
year = {2013},
isbn = {978-1-931971-01-0},
address = {San Jose, CA},
pages = {103--114},
url = {https://www.usenix.org/conference/atc13/technical-sessions/presentation/mitchell},
publisher = {USENIX Association},
month = jun
}

@article{kalia2014herd,
author = {Kalia, Anuj and Kaminsky, Michael and Andersen, David G.},
title = {Using RDMA efficiently for key-value services},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2626299},
doi = {10.1145/2740070.2626299},
abstract = {This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {295–306},
numpages = {12},
keywords = {RDMA, ROCE, infiniband, key-value stores}
}

@inproceedings{Gammo2004ComparingAE,
  title={Comparing and Evaluating epoll, select, and poll Event Mechanisms},
  author={Louay Gammo and Tim Brecht and Amol Shukla and David Pariag},
  year={2004},
  url={https://api.semanticscholar.org/CorpusID:8488207}
}

@misc{githubStefanhafdmonbench,
	author = {},
	title = {{G}it{H}ub - stefanha/fdmonbench: {F}ile {D}escriptor {M}onitoring {B}enchmark for {L}inux --- github.com},
	howpublished = {\url{https://github.com/stefanha/fdmonbench}},
	year = {},
	note = {[Accessed 08-03-2025]},
}

@INPROCEEDINGS{Huang2006MVAPICH2,
  author={Huang, W. and Santhanaraman, G. and Jin, H.-W. and Gao, Q. and Panda, D.K.},
  booktitle={Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID'06)}, 
  title={Design of High Performance MVAPICH2: MPI2 over InfiniBand}, 
  year={2006},
  volume={1},
  number={},
  pages={43-48},
  keywords={Scalability;Communication channels;Laboratories;Sun;Sockets;Computer science;Computer architecture;Message passing;Writing;High performance computing},
  doi={10.1109/CCGRID.2006.32}}

@inproceedings {xingda2018DrTM+H,
author = {Xingda Wei and Zhiyuan Dong and Rong Chen and Haibo Chen},
title = {Deconstructing {RDMA-enabled} Distributed Transactions: Hybrid is Better!},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {233--251},
url = {https://www.usenix.org/conference/osdi18/presentation/wei},
publisher = {USENIX Association},
month = oct
}

@inproceedings{graham2005OpenMPI,
author = {Graham, Richard L. and Woodall, Timothy S. and Squyres, Jeffrey M.},
title = {Open MPI: a flexible high performance MPI},
year = {2005},
isbn = {3540341412},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11752578_29},
doi = {10.1007/11752578_29},
abstract = {A large number of MPI implementations are currently available, each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the LAM/MPI, LA-MPI, FT-MPI, and PACX-MPI projects, Open MPI is an all-new, production-quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI, as well as performance results for it's point-to-point implementation.},
booktitle = {Proceedings of the 6th International Conference on Parallel Processing and Applied Mathematics},
pages = {228–239},
numpages = {12},
location = {Pozna\'{n}, Poland},
series = {PPAM'05}
}

@inproceedings {xingda2020xstore,
author = {Xingda Wei and Rong Chen and Haibo Chen},
title = {Fast {RDMA-based} Ordered {Key-Value} Store using Remote Learned Cache},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {117--135},
url = {https://www.usenix.org/conference/osdi20/presentation/wei},
publisher = {USENIX Association},
month = nov
}
%---------------------------------------------------------------------------%

